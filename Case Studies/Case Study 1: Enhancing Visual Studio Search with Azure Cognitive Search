Introduction
The team at Microsoft owns a platform for quickly creating, testing, and deploying machine learning models. This platform is the backbone of a variety of AI solutions offered in Microsoft developer tools, such as the Visual Studio Search experience and the Azure CLI. Their AI services employ complex models that routinely handle 40 requests/s with an average response time under 40ms. One of the Microsoft offerings they leverage to make this happen is Azure Cognitive Search.

Problem
The team was tasked with improving the Visual Studio Search service, which provides real-time in-tool suggestions to user queries typed in the Visual Studio search box. These queries can be in 14 language locales, and can originate from across the globe and from many active versions of Visual Studio. Their goal was to respond to queries from all the locales and active versions in under 300ms. To respond to these queries, they store tens of thousands of small documents (the units they want to search through) for each locale in Cognitive Search. They also spread out over 5 regions globally to ensure redundancy and to decrease latency.

Solution
The team leveraged Azure Cognitive Search's unique features such as natural language processing on user input, configurable keywords, and scoring parameters to drastically improve the quality of their results. They also implemented a two-tier caching strategy to cut down response time in half. Non-cached items average ~70ms, while cached items are averaged at ~30ms. The first tier is faster but localized to that service instance. The second tier is slower but can be larger and shared among other service instances in a region.

They also used a second re-ranking step after getting the initial results from Cognitive Search. This gives them more control and is something they can easily update independently of Cognitive Search. A key indicator that a result is valuable is if people actually use it. So, they generated a machine learning model that takes usage information into consideration. This file is stored in Azure Blob Storage where their service can then load in-memory. For each request, the service takes into account the userâ€™s search context, such as if a project was opened or if Visual Studio was in debugging mode. The top results from Cognitive Search are compared to the contextual usage ranked data in their model. The scores in both result lists are normalized and re-ranked, before being returned to the user.

Conclusion
The team's journey with Azure Cognitive Search has enabled them to provide better quality results to their users. They learned to have high up-time, fast responses, and cut down on costs. They also learned to choose a service tier for the future and design around it, update replicas and partitions over time to fit their service needs, check to see if shard score optimizations cause issues with their results and disable them, plan for indexer runs with lots of documents to take time and prod them along, leverage Incremental Caching in their index, employ exponential backoff for sending request, use their own (correctly configured) rate limiting, cache what results they can at the service level, and consider post-processing their results.

Source
You can read the full case study here [https://techcommunity.microsoft.com/t5/ai-cognitive-services-blog/case-study-effectively-using-cognitive-search-to-support-complex/ba-p/2804078].
